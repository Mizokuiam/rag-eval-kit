# rag_eval_kit/core.py
import json
import statistics
from typing import List, Dict, Any, Callable, Tuple, Optional, TypedDict, Set

# --- Default Prompts for LLM-as-a-judge ---

DEFAULT_FAITHFULNESS_PROMPT_TEMPLATE = """
System: You are an expert evaluator assessing the faithfulness of an answer generated by a Retrieval-Augmented Generation (RAG) system. Compare the generated 'Answer' ONLY against the provided 'Retrieved Context'. The answer must not contradict the context and should only contain information present in the context. Ignore your own knowledge.

User:
Retrieved Context:
---
{context}
---

Generated Answer:
---
{answer}
---

Question: Is the 'Generated Answer' faithful to the 'Retrieved Context'?
Answer ONLY with 'SUPPORTED' if the answer is fully supported by the context, or 'UNSUPPORTED' if it contains information not found in or contradicting the context.
"""

DEFAULT_RELEVANCY_PROMPT_TEMPLATE = """
System: You are an expert evaluator assessing the relevancy of an answer generated by a Retrieval-Augmented Generation (RAG) system. Compare the 'Generated Answer' against the original 'Question'. The answer should directly and completely address the question.

User:
Question:
---
{question}
---

Generated Answer:
---
{answer}
---

Question: Is the 'Generated Answer' relevant and directly addressing the 'Question'?
Answer ONLY with 'RELEVANT' or 'IRRELEVANT'.
"""

# --- Type Definitions ---

class RetrievalResult(TypedDict):
    retrieved_ids: List[str]
    retrieved_content: List[str] # Content corresponding to IDs

class EvaluationResult(TypedDict):
    context_precision: float
    context_recall: float
    faithfulness: Optional[float] # Use Optional for cases where generation fails
    answer_relevancy: Optional[float] # Use Optional for cases where generation fails

# Define types for the user-provided functions
RetrieverFunc = Callable[[str], RetrievalResult]
GeneratorFunc = Callable[[str, List[str]], str] # Takes question, list of context strings
LLMClientFunc = Callable[[str], str] # Takes prompt string, returns LLM response string

# --- Metric Calculations ---

def calculate_retrieval_metrics(retrieved_ids: List[str], ground_truth_ids: List[str]) -> Tuple[float, float]:
    """Calculates Context Precision and Recall."""
    retrieved_set = set(retrieved_ids)
    ground_truth_set = set(ground_truth_ids)

    if not retrieved_set and not ground_truth_set:
        return 1.0, 1.0 # Both empty, perfect match conceptually
    if not retrieved_set:
        return 0.0, 0.0 # Nothing retrieved, precision=0, recall can't be >0
    if not ground_truth_set:
         # Nothing needed, precision is debatable, let's say 1.0 if nothing retrieved, else 0.0?
         # Or maybe it depends? Let's default to 0.0 precision if GT is empty but we retrieved something.
         # Recall is 1.0 as we didn't miss any required documents.
        return 0.0 if retrieved_set else 1.0, 1.0

    intersection = retrieved_set.intersection(ground_truth_set)

    precision = len(intersection) / len(retrieved_set)
    recall = len(intersection) / len(ground_truth_set)

    return precision, recall

def evaluate_faithfulness(
    question: str,
    retrieved_context: List[str],
    generated_answer: str,
    llm_client: LLMClientFunc,
    prompt_template: str = DEFAULT_FAITHFULNESS_PROMPT_TEMPLATE
) -> Optional[float]:
    """Evaluates Faithfulness using LLM-as-a-judge."""
    if not generated_answer: # Handle cases where generation failed
        return None
    context_str = "\n---\n".join(retrieved_context)
    prompt = prompt_template.format(context=context_str, answer=generated_answer)
    try:
        response = llm_client(prompt)
        # Basic parsing, robust checks needed for production
        response_clean = response.strip().upper()
        if "UNSUPPORTED" in response_clean:
            return 0.0
        elif "SUPPORTED" in response_clean:
            return 1.0
        else:
            print(f"Warning: Faithfulness LLM response ambiguous: '{response}'")
            return None # Ambiguous response
    except Exception as e:
        print(f"Error during Faithfulness LLM call: {e}")
        return None

def evaluate_relevancy(
    question: str,
    generated_answer: str,
    llm_client: LLMClientFunc,
    prompt_template: str = DEFAULT_RELEVANCY_PROMPT_TEMPLATE
) -> Optional[float]:
    """Evaluates Answer Relevancy using LLM-as-a-judge."""
    if not generated_answer: # Handle cases where generation failed
        return None
    prompt = prompt_template.format(question=question, answer=generated_answer)
    try:
        response = llm_client(prompt)
        # Basic parsing
        response_clean = response.strip().upper()
        if "IRRELEVANT" in response_clean:
            return 0.0
        elif "RELEVANT" in response_clean:
            return 1.0
        else:
            print(f"Warning: Relevancy LLM response ambiguous: '{response}'")
            return None # Ambiguous response
    except Exception as e:
        print(f"Error during Relevancy LLM call: {e}")
        return None

# --- Main Evaluation Loop ---

def run_evaluation(
    dataset_path: str,
    retriever_func: RetrieverFunc,
    generator_func: GeneratorFunc,
    llm_client_func: LLMClientFunc,
) -> Dict[str, float]:
    """
    Runs the evaluation pipeline over a dataset.

    Args:
        dataset_path: Path to the JSONL dataset file. Each line should be a JSON object
                      with keys: 'question', 'ground_truth_context_ids' (List[str]),
                      'ground_truth_answer' (str).
        retriever_func: A function that takes a question (str) and returns a RetrievalResult dict.
        generator_func: A function that takes a question (str) and retrieved context (List[str])
                        and returns the generated answer (str).
        llm_client_func: A function that takes a prompt (str) and returns the LLM response (str),
                         used for LLM-as-a-judge evaluations.

    Returns:
        A dictionary containing the average scores for all metrics.
    """
    results: List[EvaluationResult] = []
    total_items = 0

    try:
        with open(dataset_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                try:
                    data_item = json.loads(line.strip())
                    question = data_item['question']
                    ground_truth_ids = data_item['ground_truth_context_ids']
                    # ground_truth_answer = data_item['ground_truth_answer'] # Not used in default metrics yet

                    print(f"\nProcessing item {i+1}: {question[:80]}...")
                    total_items += 1

                    # 1. Retrieval Step
                    try:
                        retrieval_output = retriever_func(question)
                        retrieved_ids = retrieval_output['retrieved_ids']
                        retrieved_content = retrieval_output['retrieved_content']
                        print(f"  Retrieved {len(retrieved_ids)} docs: {retrieved_ids}")
                    except Exception as e:
                        print(f"  Error during retrieval: {e}")
                        # Assign defaults or skip item if retrieval fails critically
                        retrieved_ids = []
                        retrieved_content = []
                        # Decide if you want to proceed without context or skip

                    # Calculate Retrieval Metrics
                    precision, recall = calculate_retrieval_metrics(retrieved_ids, ground_truth_ids)
                    print(f"  Context Precision: {precision:.4f}, Context Recall: {recall:.4f}")

                    # 2. Generation Step
                    generated_answer = "" # Default empty if generation fails
                    try:
                        generated_answer = generator_func(question, retrieved_content)
                        print(f"  Generated Answer: {generated_answer[:100]}...")
                    except Exception as e:
                        print(f"  Error during generation: {e}")
                        # Generation failed, metrics depending on it will be None


                    # Calculate Generation Metrics (using LLM-as-a-judge)
                    faithfulness_score = evaluate_faithfulness(question, retrieved_content, generated_answer, llm_client_func)
                    relevancy_score = evaluate_relevancy(question, generated_answer, llm_client_func)
                    print(f"  Faithfulness: {faithfulness_score}, Answer Relevancy: {relevancy_score}")

                    results.append({
                        "context_precision": precision,
                        "context_recall": recall,
                        "faithfulness": faithfulness_score,
                        "answer_relevancy": relevancy_score
                    })

                except json.JSONDecodeError as e:
                    print(f"Skipping invalid JSON line {i+1}: {e}")
                except KeyError as e:
                     print(f"Skipping line {i+1} due to missing key: {e}")
                except Exception as e:
                    print(f"Unexpected error processing line {i+1}: {e}")

    except FileNotFoundError:
        print(f"Error: Dataset file not found at {dataset_path}")
        return {}
    except Exception as e:
        print(f"Error reading dataset file: {e}")
        return {}

    # --- Aggregate Results ---
    if not results:
        print("No results were generated.")
        return {}

    # Helper to calculate mean ignoring Nones
    def robust_mean(scores: List[Optional[float]]) -> float:
        valid_scores = [s for s in scores if s is not None]
        return statistics.mean(valid_scores) if valid_scores else 0.0

    avg_precision = robust_mean([r['context_precision'] for r in results])
    avg_recall = robust_mean([r['context_recall'] for r in results])
    avg_faithfulness = robust_mean([r['faithfulness'] for r in results])
    avg_relevancy = robust_mean([r['answer_relevancy'] for r in results])

    summary = {
        "total_items_processed": total_items,
        "average_context_precision": avg_precision,
        "average_context_recall": avg_recall,
        "average_faithfulness": avg_faithfulness,
        "average_answer_relevancy": avg_relevancy,
    }

    print("\n--- Evaluation Summary ---")
    for key, value in summary.items():
        print(f"{key}: {value:.4f}" if isinstance(value, float) else f"{key}: {value}")

    return summary